{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formation PyTorch : les bases pour être autonome \n",
    "#### 19 avril 2023de 9h à 17h à l'OMP (salle Coriolis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1\n",
    "## Manipuler les objets basiques de PyTorch : tenseurs, paramètres, modèles... \n",
    "\n",
    "Dans cette première partie, nous allons : \n",
    " * Découvrir et manipuler les éléments de base de PyTorch,\n",
    " * Découvrir les bases de l'optimisation avec PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les tenseurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tenseur est l'objet incontournable de PyTorch, l'équivalent du array pour la librairie numpy. La très grande majorité des opérations effectuées avec PyTorch sont effectuées sur des tenseurs. On va voir ci-dessous une liste non exhaustive des opérations possibles sur les tenseurs.\n",
    "\n",
    "Pour plus d'informations, regardez la documentation : https://pytorch.org/docs/stable/tensors.html?highlight=torch+tensor#torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([\n",
    "    [2, 1.5, 3],\n",
    "    [4, 2, 4.5]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(X, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.view(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.repeat(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros((2,2))\n",
    "torch.ones_like(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramètres et rétro-propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les paramètres sont des objets dont les valeurs ont vocation à être optimisées, comme par exemple les paramètres d'un réseau de neurones.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html?highlight=parameter#torch.nn.parameter.Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "X = torch.ones(10,1) # un tenseur classique \n",
    "W = Parameter(torch.tensor([[2.]]), requires_grad=True) # un paramètre\n",
    "Y = torch.mm(X, W) # produit matriciel\n",
    "z = torch.prod(Y) # produit des coordonnées "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.mm(X, W)` effectue le produit matriciel de X et de W: $Y = X W$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytiquement, on peut calculer le gradient de $z$ par rapport à $W$ :\n",
    "    \n",
    "\\begin{align}\n",
    "    \\frac{\\partial z}{\\partial W} & = \\frac{\\partial z}{\\partial Y} \\frac{\\partial Y}{\\partial W} \\\\\n",
    "     & = \\big[\\frac{\\partial z}{\\partial Y_1} \\ldots \\frac{\\partial z}{\\partial Y_{10}}\\big] \\big[\\frac{\\partial Y_1}{\\partial Y_W} \\ldots \\frac{\\partial Y_{10}}{\\partial W} \\big]^T \\\\ \n",
    "     & = \\big[\\prod_{i=2}^{10} Y_i \\ldots \\prod_{i=1}^{9} Y_i\\big] \\big[X_1 \\ldots X_{10}\\big]^T\n",
    "\\end{align}\n",
    "    \n",
    "Finalement, $\\large{\\frac{\\partial z}{\\partial W}(W) = 5120}$.\n",
    "\n",
    "C'est en fait exactement ce que fait PyTorch : à chaque opération réalisée à partir de paramètres, l'attribut `grad_fn` garde en mémoire la fonction qui permet d'évaluer le gradient. Par exemple, `Y.grad_fn` permet de calculer le gradient $\\large{\\frac{\\partial Y}{\\partial W}}$.\n",
    "\n",
    "C'est ce que l'on appelle la diférentiation automatique, qui utilise, comme nous l'avons fait analytiquement, le théorème de la \"chain rule\", ou théorème de dérivation des fonctions composées en français."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W.grad_fn)\n",
    "print(Y.grad_fn)\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode `backward` permet de calculer le gradient de $z$ par rapport à $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward() \n",
    "print(W.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, PyTorch ne calcule pas le gradient de Y car ce n'est pas une feuille de l'arbre de calcul (voir illustratio ci-dessus). Si on le souhaite, il faut utiliser la méthode `retain_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.ones(10,1)\n",
    "W = Parameter(torch.tensor([[2.]]), requires_grad=True)\n",
    "Y = torch.mm(X, W)\n",
    "Y.retain_grad()\n",
    "z = torch.prod(Y)\n",
    "z.backward()\n",
    "\n",
    "print(Y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On retrouve bien le gradient calculé analytiquement. \n",
    "Voyons si on peut également calculer la jacobienne de $Y$ par rapport à $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.ones(10,1)\n",
    "W = Parameter(torch.tensor([[2.]]), requires_grad=True)\n",
    "Y = torch.mm(X, W)\n",
    "Y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, $Y$ n'est pas scalaire. La méthode `backward` ne peut pas implicitement calculer la jacobienne de $Y$ par rapport à $W$. En fait, PyTorch n'est, de base, pas fait pour calculer des quantitités comme des jacobiennes ou des hessiennes. Si vous en avez l'utilité, vous pouvez aller voir ce tutoriel : https://pytorch.org/functorch/stable/notebooks/jacobians_hessians.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessous, on va tout de même voir comment calculer la jacobienne $\\large{\\frac{\\partial Y}{\\partial W}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X, W):\n",
    "    return torch.mm(X, W)\n",
    "\n",
    "X = torch.arange(1, 11).reshape((10,1)).float()\n",
    "W = Parameter(torch.tensor([[2.]]), requires_grad=True)\n",
    "\n",
    "# Calculer les lignes de la matrice jacobienne\n",
    "jacobian_rows = ...\n",
    "\n",
    "jacobian = torch.stack(jacobian_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On retrouve bien ce que l'on avait calculé analytiquement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation des paramètres "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va voir ci-dessous quels outils utiliser pour l'optimisation des paramètres.\n",
    "\n",
    "https://pytorch.org/docs/stable/optim.html?highlight=optimizer#torch.optim.Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.linspace(0, 10, 100).unsqueeze(1)\n",
    "Y = 2.5 + 3*X + 3*torch.randn(100,1)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "# Complétez la définition de a, le coefficient directeur, et b, l'ordonnée à l'origine\n",
    "a = ...\n",
    "b = ...\n",
    "\n",
    "# Complétez la définition de l'optimiseur : \n",
    "# https://pytorch.org/docs/stable/generated/torch.optim.SGD.html?highlight=sgd#torch.optim.SGD\n",
    "optimizer = torch.optim.SGD(..., lr=1e-2)\n",
    "\n",
    "for epoch in range(10):\n",
    "    # Complétez le calcul de y_pred\n",
    "    y_pred = ...\n",
    "    mse = F.mse_loss(Y, y_pred)\n",
    "    mse.backward() # On calcule les gradients de mse par rapport à a et b \n",
    "    optimizer.step() # On met à jour les valeurs de a et b \n",
    "    optimizer.zero_grad() # On 'remet' les gradients à zéro pour la prochaine epoch \n",
    "    print(f'=== Epoch {epoch} ===  ')\n",
    "    print(f'MSE: {mse.item():.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = torch.mm(X, a) + b \n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.scatter(X, Y)\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les modèles sont les objets de PyTorch qui définissent les réseaux de neurones.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self, x_dim):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.linear = torch.nn.Linear(x_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearModel(X.shape[1])\n",
    "linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in linear_model.named_parameters():\n",
    "    print(param[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.linear.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(..., lr=1e-2)\n",
    "\n",
    "for epoch in range(10):\n",
    "    y_pred = ...\n",
    "    mse = F.mse_loss(Y, y_pred)\n",
    "    mse.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f'=== Epoch {epoch} ===  ')\n",
    "    print(f'MSE: {mse.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = ...\n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.scatter(X, Y)\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les data loader sont des objets de PyTorch pour charger les données de manière à faciliter l'optimisation par \"Batch stochastic gradient descent\".\n",
    "\n",
    "https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.linspace(0, 10, 100).unsqueeze(1)\n",
    "Y = 2.5 + 3*X + 3*torch.randn((100,1))\n",
    "\n",
    "data = torch.utils.data.TensorDataset(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(data, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(linear_model..., lr=1e-2)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch, y in loader:\n",
    "        y_pred = ...\n",
    "        mse = F.mse_loss(y, y_pred)\n",
    "        mse.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(f'=== Epoch {epoch} ===  ')\n",
    "    print(f'MSE: {mse.item():.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
