{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formation PyTorch : les bases pour être autonome \n",
    "#### 19 avril 2023 de 9h à 17h à l'OMP (salle Coriolis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 3\n",
    "\n",
    "Dans cette troisième partie, nous allons voir comment adapter des modules PyTorch pour créer ses propres couches et comment définir ses propres fonctions de régularisation.\n",
    "\n",
    "## Définir ses propres couches de convolution \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les noyaux de convolution de la première couche d'AlexNet (voir partie 2) ressemblent à des filtres de Gabor. On pourrait initialiser les noyaux de convolution comme des filtres de Gabor ou même optimiser des filtres de Gabor comme proposé dans [2] plutôt que des noyaux classiques.\n",
    "\n",
    "Dans cette sous-partie, on va définir notre propre couche de filtre de Gabor. Précisémment, on va implémenter la partie réelle d'un filtre de Gabor qui est définie comme suit : \n",
    "\n",
    "$$g(x,y; \\lambda, \\psi, \\sigma, \\gamma, \\theta) = exp(-\\frac{x'^2 + \\gamma^2 \\cdot y'^2}{2\\sigma^2}) \\cdot cos(2\\pi \\frac{x'}{\\lambda} + \\psi)$$\n",
    "avec \n",
    "$$x' = x \\: cos \\: \\theta + y \\: sin \\: \\theta ; \\:\\:\\: y' = -x \\: sin \\: \\theta + y \\: cos \\: \\theta$$ \n",
    "\n",
    "où $\\lambda > 0$, $\\psi \\in [0, 2\\pi]$, $\\sigma > 0$ et $\\gamma >0$ sont des paramètres à optimiser. $\\theta \\in [0, \\pi[$ prendra plusieurs valeurs déterminées pour obtenir des filtres avec différentes orientations.\n",
    "\n",
    "Le code ci-dessous est fortement inspiré du code au lien suivant : https://gist.github.com/DerThorsten/7117b9b7a41da4e0a13d6500f9a1b657\n",
    "\n",
    "[2] Alekseev, Andrey, and Anatoly Bobe. \"GaborNet: Gabor filters with learnable parameters in deep convolutional neural network.\" 2019 International Conference on Engineering and Telecommunication (EnT). IEEE, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En fait, on souhaiterait conserver les propriétés et fonctions d'une couche de convolution classique, mais en redéfinissant uniquement les poids de la convolution. On va donc écire une classe qui hérite de torch.nn.modules.conv.Conv2d.\n",
    "\n",
    "AlexNet avait 64 convolutions par canal de dimensions spatiales (11x11) pour sa première couche. On fait la même chose ici : pour chaque canal, on apprend 64 filtres de Gabor de taille (11x11). Afin d'avoir des filtres invariants aux rotations, on fait varier le paramètre $\\theta$, comme indiqué ci-dessus, pour 8 valeurs entre $0$ et $2\\pi$. Pour un $\\theta$ donné, il reste donc 8 couples différents de paramètres $(\\lambda, \\psi, \\sigma, \\gamma)$ à optimiser. \n",
    "\n",
    "On a donc des filtres de Gabor de tailles $(8 \\times 8 \\times 3 \\times 11 \\times 11)$ où la première dimension correspond aux couples de paramètres $(\\lambda, \\psi, \\sigma, \\gamma)$, la deuxième dimension correspond à $\\theta$, la troisième dimension correspond aux canaux RGB et les deux dernières dimensions correspondent aux dimensions spatiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaborFilters(nn.modules.conv.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, dilation=1, groups=1, bias=False,\n",
    "                 padding_mode='zeros', device=None, dtype=None, n_thetas=5):\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.bias_ = bias\n",
    "        self.padding_mode = padding_mode\n",
    "        self.device = device\n",
    "        self.dtype=dtype\n",
    "        self.n_thetas = self.out_channels//8\n",
    "        \n",
    "        # A compléter\n",
    "        super().__init__(...)\n",
    "                \n",
    "        # Initialisation des paramètres lambda, psi, sigma et gamma\n",
    "        lambda_init_values = torch.rand(self.out_channels//self.n_thetas, 1, self.in_channels, 1, 1)*32\n",
    "        psi_init_values = torch.rand(self.out_channels//self.n_thetas, 1, self.in_channels, 1, 1)*2*math.pi\n",
    "        sigma_init_values = torch.rand(self.out_channels//self.n_thetas, 1, self.in_channels, 1, 1)*10\n",
    "        gamma_init_values = torch.rand(self.out_channels//self.n_thetas, 1, self.in_channels, 1, 1)\n",
    "        \n",
    "        self.lambda_ = torch.nn.Parameter(lambda_init_values, requires_grad=True)\n",
    "        self.psi = torch.nn.Parameter(psi_init_values,requires_grad=True)\n",
    "        self.sigma = torch.nn.Parameter(sigma_init_values, requires_grad=True)\n",
    "        self.gamma = torch.nn.Parameter(gamma_init_values, requires_grad=True)\n",
    "        \n",
    "        # On fixe les valeurs de theta\n",
    "        thetas = torch.linspace(0., math.pi*(self.n_thetas-1)/self.n_thetas, self.n_thetas)\n",
    "        # A compléter\n",
    "        self.register_buffer(...)\n",
    "        \n",
    "    @property\n",
    "    def weight(self):\n",
    "        # A compléter\n",
    "        ...\n",
    "\n",
    "    def gabor_filter(self):\n",
    "        x = torch.arange(self.kernel_size[0], dtype=torch.float32) -  (self.kernel_size[0] - 1)/2\n",
    "        x = x.view(1, 1, 1, 1, self.kernel_size[0])\n",
    "        y = torch.arange(self.kernel_size[1], dtype=torch.float32) -  (self.kernel_size[1] - 1)/2\n",
    "        y = y.view(1, 1, 1, self.kernel_size[1], 1)\n",
    "        \n",
    "        thetas = self.thetas.view(1, -1, 1, 1, 1)\n",
    "\n",
    "        x_ =  x * torch.cos(thetas) + y * torch.sin(thetas)\n",
    "        y_ = -x * torch.sin(thetas) + y * torch.cos(thetas)\n",
    "\n",
    "        gb = torch.exp(-0.5 * ((x_ ** 2 + self.gamma**2 * y_ ** 2) / (1e-2+self.sigma) ** 2)) \\\n",
    "                 * torch.cos(2.0 * math.pi  * x_ / (1e-2+F.relu(self.lambda_)) + self.psi)\n",
    "        \n",
    "        gb = gb.view(self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1])\n",
    "\n",
    "        return gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit un modèle équivalent au TinyAlexNet de la partie 2 mais on substitue la première couche de convolution à une couche de filtres de Gabor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaborAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, dropout_rate=0):\n",
    "        super(GaborAlexNet, self).__init__()\n",
    "        \n",
    "        self.features_gabor = GaborFilters(3, 64, kernel_size=11, stride=4, padding=2)\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 1, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(36, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "        \n",
    "        #torch.manual_seed(0)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def extract_features(self, inputs):\n",
    "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
    "        x = self.features(inputs)\n",
    "        return x\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.features_gabor(inputs)\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaborAlexNet(num_classes=2)\n",
    "model.features_gabor.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'state_dict': model.state_dict()}, 'gabor_net.pth.tar')\n",
    "checkpoint = torch.load('gabor_net.pth.tar')\n",
    "checkpoint['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons les filtres de Gabor à l'initialisation du modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaborAlexNet(num_classes=2)\n",
    "\n",
    "fig, ax = plt.subplots(8,8, figsize=(10,10))\n",
    "k = 0\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        # A compléter \n",
    "        ax[i,j].imshow(..., cmap='gray')\n",
    "        ax[i,j].set_xticks([])\n",
    "        ax[i,j].set_yticks([])\n",
    "        k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifions que nous avons bien défini les filtres de Gabor et que nous pouvons optimiser ses paramètres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaborAlexNet(num_classes=2)\n",
    "print(model.features_gabor.lambda_.grad)\n",
    "x = torch.randn((1, 3, 125, 125)) # Image aléatoire\n",
    "out = model(x)\n",
    "loss = F.cross_entropy(out, torch.ones(1).long()) # Label aléatoire\n",
    "loss.backward()\n",
    "model.features_gabor.lambda_.grad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régularisation\n",
    "\n",
    "On peut facilement régulariser la norme L2 des poids d'un modèle avec l'argument \"weight_decay\" des optimizer Pytorch. Regardez par exemple les arguments de Adam (https://pytorch.org/docs/stable/generated/torch.optim.Adam.html?highlight=weight_decay). Néanmoins, on pourrait vouloir définir d'autres types de régularisation. \n",
    "\n",
    "Ici, on va régulariser la norme L1 des poids des couches denses de GaborAlexNet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1_regularization(submodel):\n",
    "    \"\"\"\n",
    "    Arg:\n",
    "        * submodel: a torch.nn.modules.container.Sequential object\n",
    "    Output:\n",
    "        * regularization term\n",
    "    \"\"\"\n",
    "    # A compléter\n",
    "    ...\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vérifie que la régularisation fonctionne bien :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaborAlexNet(num_classes=2)\n",
    "print(model.classifier[1].weight.grad)\n",
    "reg = L1_regularization(model.classifier)\n",
    "reg.backward()\n",
    "model.classifier[1].weight.grad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application au problème de la partie 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "img = np.load('img.npy')\n",
    "gt = np.load('gt.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {'ignored_labels': [0], 'patch_size': 125}\n",
    "dataset = Dataset(img, gt, hyperparams)\n",
    "train_dataset, val_dataset = ...\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "num_epochs = 1\n",
    "best_val = np.inf\n",
    "optimizer = torch.optim.Adam([{'params': ..., 'lr':5e-3}, \n",
    "                              {'params': ..., 'lr':1e-3}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    y_pred, y_true = [], []\n",
    "    for patch, label in tqdm(train_loader):\n",
    "        patch, label = patch.to(device), label.to(device)-1\n",
    "        logits = model(patch) # model;forward(patch)\n",
    "        loss = F.cross_entropy(logits, label)\n",
    "        loss.backward() # Calcul du gradient \n",
    "        optimizer.step() # Mise à jour des paramètres\n",
    "        optimizer.zero_grad() # Mise à zéro du gradient\n",
    "        \n",
    "        y_pred.extend(torch.argmax(logits, dim=-1).cpu())\n",
    "        y_true.extend(label.cpu())\n",
    "          \n",
    "    train_accuracy = accuracy_score(y_pred, y_true)\n",
    "    train_f1 = f1_score(y_pred, y_true)\n",
    "    \n",
    "    y_pred, y_true = [], []\n",
    "    for patch, label in tqdm(val_loader):\n",
    "        patch, label = patch.to(device), label.to(device)-1\n",
    "        with torch.no_grad():\n",
    "            logits = model(patch)\n",
    "        val_loss = F.cross_entropy(logits, label)\n",
    "        \n",
    "        y_pred.extend(torch.argmax(logits, dim=-1).cpu())\n",
    "        y_true.extend(label.cpu())\n",
    "        \n",
    "    val_accuracy = accuracy_score(y_pred, y_true)\n",
    "    val_f1 = f1_score(y_pred, y_true)\n",
    "    \n",
    "    if val_loss.item() < best_val:\n",
    "        best_val = val_loss.item()\n",
    "        torch.save({'epoch': epoch, \n",
    "                    'best_loss': best_val, \n",
    "                    'state_dict': model.state_dict()}, 'best_model.pth.tar')\n",
    "        \n",
    "    print('[Train] \\t Loss: {:.4f} | Accuracy: {:.4f} | F1 score: {:.4f}'.format(\\\n",
    "                                            loss.item(), train_accuracy, train_f1))\n",
    "    print('[Val] \\t Loss: {:.4f} | Accuracy: {:.4f} | F1 score: {:.4f}'.format(\\\n",
    "                                          val_loss.item(), val_accuracy, val_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'optimisation converge beaucoup moins bien avec les filtres de Gabor. L'optimisation, en particulier, semble beaucoup plus sensible à l'intialisation des paramètres des filtres. Il faudrait creuser davantage pour mieux régler les hyper-paramètres du modèle et de l'optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(8,8, figsize=(10,10))\n",
    "k = 0\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        ax[i,j].imshow(..., cmap='gray')\n",
    "        ax[i,j].set_xticks([])\n",
    "        ax[i,j].set_yticks([])\n",
    "        k += 1\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
