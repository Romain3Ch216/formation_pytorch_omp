{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formation PyTorch : les bases pour être autonome \n",
    "#### 3 novembre 2022 de 9h à 17h à l'OMP (salle Coriolis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2\n",
    "## Adapter un modèle pré-entrainé \n",
    "\n",
    "Dans cette deuxième partie, nous travaillerons avec un modèle classique dans la litérature du machine learning : AlexNet [1], un réseau de neurones profond à convolutions pour la classification d'images naturelles (voir exemples d'images des données ImageNet ci-dessous).\n",
    "\n",
    "Nous allons : \n",
    " * Définir et adapter un **modèle PyTorch** de AlexNet,\n",
    " * Charger dans le modèle des paramètres **pré-entrainés**,\n",
    " * **Visualiser** les noyaux de convolution,\n",
    " * Créer un torch.utils.data.Dataset adapté à notre application,\n",
    " * **Fine-tuner** le modèle.\n",
    " \n",
    "On utilisera un extrait de l'image RGB à haute résolution spatiale de Houston [2]. Cet extrait peut être téléchargé ici : https://nextcloud.isae.fr/index.php/s/WmjQPyH3g2EK33x\n",
    "\n",
    "\n",
    "[1] Krizhevsky, Alex et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM 60 (2012): 84 - 90.\n",
    "\n",
    "[2] Saurabh Prasad, Bertrand Le Saux, Naoto Yokoya, Ronny Hansch, December 18, 2020, \"2018 IEEE GRSS Data Fusion Challenge – Fusion of Multispectral LiDAR and Hyperspectral Data\", IEEE Dataport, doi: https://dx.doi.org/10.21227/jnh9-nz89.\n",
    "\n",
    "<img src=\"image_net.png\" width=500 height=200 />\n",
    "\n",
    "<center>Exemples d'images de ImageNet</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition du modèle AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"alex_net.png\" width=700 height=300 />\n",
    "\n",
    "\n",
    "Représentation schématique du modèle AlexNet (Source : Llamas, Jose & Lerones, Pedro & Medina, Roberto & Zalama, Eduardo & Gómez-García-Bermejo, Jaime. (2017). Classification of Architectural Heritage Images Using Deep Learning Techniques. Applied Sciences. 7. 992. 10.3390/app7100992.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet utilise également des opérations de padding et de pooling, respectivement illustrées ci-dessous: \n",
    "<img src=\"padding_pooling.png\" width=700 height=300 />\n",
    "\n",
    "(Source : Audebert, N. (2018). Classification de données massives de télédétection (Doctoral dissertation, Université Bretagne Sud))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions d'activation utilisées dans AlexNet sont des fonctions ReLU (Rectified Linear Unit) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-10, 10, 100)\n",
    "y = torch.nn.functional.relu(x)\n",
    "fig = plt.figure()\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('ReLU(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit ci-dessous l'architecture du modèle AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    \"\"\"\n",
    "    AlexNet model architecture from the <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    Credits to https://github.com/Lornatang/AlexNet-PyTorch \n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10, dropout_rate=0):\n",
    "        # Notre modèle, comme tous les réseaux de neurones dans PyTorch,\n",
    "        # hérite de la classe générique nn.Module.\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        # A l'initialisation, on définit les couches du modèle, les fonctions\n",
    "        # d'activation, etc. \n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "        # On définit aussi les distributions de probabilité selon lesquelles\n",
    "        # les paramètres du modèle vont être initialisés.\n",
    "\n",
    "        for m in self.modules():\n",
    "            # Les paramètres sont initialisés différemment \n",
    "            # selon le type de couche.\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def extract_features(self, inputs):\n",
    "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
    "        x = self.features(inputs)\n",
    "        return x\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.features(inputs)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.named_parameters() renvoie un générateur de tuples contenant le nom et la valeur des paramètres d'une instance de nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(x[0], x[1].shape) for x in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.parameters() renvoie un générateur contenant uniquement la valeur des paramètres d'une instance de nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[param.shape for param in model.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons plus en détails le fonctionnement d'une convolution 2D :\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv#torch.nn.Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.features[0].weight.shape)\n",
    "print(model.features[0].bias.shape)\n",
    "x = torch.ones((1, 3, 100, 100))\n",
    "out = model.features[0](x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_params(model):\n",
    "    n = 0\n",
    "    for param in model.parameters():\n",
    "        n += param.shape.numel()\n",
    "    return n\n",
    "\n",
    "print(\"{:.2e}\".format(n_params(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charger des paramètres pré-entrainés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On charge les paramètres d'AlexNet optimisés sur ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "pretrained_weights = torch.load('../pretrained_alex_net.pth.tar', \n",
    "                                map_location=device) # precise device on which the model is\n",
    "model.load_state_dict(pretrained_weights['state_dict'])\n",
    "del pretrained_weights # Delete the weights to clear memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observer les filtres de convolution peut nous donner un peu d'intuition sur le fonctionnement du modèle. On va regarder les filtres de la première couche de convolution pour le canal rouge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(8,8, figsize=(10,10))\n",
    "k = 0\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        # A compléter \n",
    "        ax[i,j].imshow(....cpu(), cmap='gray')\n",
    "        ax[i,j].set_xticks([])\n",
    "        ax[i,j].set_yticks([])\n",
    "        k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données\n",
    "\n",
    "On va s'intéresser à un extrait de l'image satellite RGB à haute résolution spatiale de Houston [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.warp import reproject, Resampling\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise ici la librairie rasterio pour recaler la vérité terrain (les annotations de pixels) avec l'image satellite, qui ne sont pas aux mêmes résolutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open('../data/UH_NAD83_272056_3289689.tif') as img:\n",
    "    window = Window(0, 0, 1000, 1000)\n",
    "    data = img.read(window=window)\n",
    "    \n",
    "with rasterio.open('../data/2018_IEEE_GRSS_DFC_GT_TR.tif') as gt_:\n",
    "    gt = np.zeros(img.shape, dtype=np.uint8)\n",
    "    reproject(\n",
    "        source=gt_.read(1),\n",
    "        destination=gt,\n",
    "        src_transform=gt_.transform,\n",
    "        src_crs=gt_.crs,\n",
    "        dst_transform=img.transform,\n",
    "        dst_crs=img.crs,\n",
    "        resampling=Resampling.nearest)\n",
    "    \n",
    "img = data.transpose(1, 2, 0)\n",
    "gt = gt[:1000, :1000]\n",
    "for i, class_id in enumerate(np.unique(gt)):\n",
    "    gt[class_id == gt] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va considérer dans la suite deux classes seulement : les bâtiments (classe 1) et la voirie (classe 2). Les pixels sans vérité terrain sont annotés par des zéros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,10))\n",
    "ax[0].imshow(img)\n",
    "ax1 = ax[1].imshow(gt)\n",
    "fig.colorbar(ax1, ax=ax, shrink=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du Dataset PyTorch\n",
    "\n",
    "\n",
    "Pour optimiser un modèle PyTorch par descente de gradient stochastique par batchs, on utilise deux objets :\n",
    "\n",
    " - un torch.utils.data.Dataset pour parcourir les exemples d'apprentissage,\n",
    " - un torch.utils.data.DataLoader pour faire des batchs d'exemples.\n",
    " \n",
    "On souhaiterait donner en entrée du modèle AlexNet des imagettes de 125 x 125 pixels, pour obtenir en sortie la prédiction de la classe du pixel au centre de l'imagette. \n",
    "\n",
    "Pour cela, on va construire notre propre torch.utils.data.Dataset : \n",
    "https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset\n",
    "\n",
    "Remarque : le but ici est pédagogique, on utiliserait des architectures de réseaux plus adaptées en réalité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, gt, hyperparams):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: 3D hyperspectral image\n",
    "            gt: 2D array of labels\n",
    "            patch_size: int, size of the spatial neighbourhood\n",
    "            ignored_labels: list of ints, labels to ignore\n",
    "\n",
    "        \"\"\"\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.label = gt\n",
    "        self.patch_size = hyperparams[\"patch_size\"]\n",
    "        self.ignored_labels = set(hyperparams[\"ignored_labels\"])        \n",
    "        self.height = data.shape[0]\n",
    "        self.width = data.shape[1]\n",
    "\n",
    "        mask = np.ones_like(gt)\n",
    "        for l in self.ignored_labels:\n",
    "            mask[gt == l] = 0\n",
    "\n",
    "        x_pos, y_pos = np.nonzero(mask)\n",
    "        p = self.patch_size // 2\n",
    "        if p > 0:\n",
    "            self.indices = np.array(\n",
    "                [\n",
    "                    (x, y)\n",
    "                    for x, y in zip(x_pos, y_pos)\n",
    "                    if x >= p and x < data.shape[0] - p and y >= p and y < data.shape[1] - p\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.indices = np.array(\n",
    "                [\n",
    "                    (x, y)\n",
    "                    for x, y in zip(x_pos, y_pos)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "        self.labels = [self.label[x, y] for x, y in self.indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        # A compléter \n",
    "        ... \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # A compléter\n",
    "        x, y = ... \n",
    "        x1, y1 = x - self.patch_size // 2, y - self.patch_size // 2\n",
    "        x2, y2 = x1 + self.patch_size, y1 + self.patch_size\n",
    "        \n",
    "        # A compléter\n",
    "        data = ...\n",
    "        label = ...\n",
    "\n",
    "        # Copie de data et label dans des npy array\n",
    "        data = np.asarray(np.copy(data).transpose((2, 0, 1)), dtype=\"float32\")\n",
    "        label = np.asarray(np.copy(label), dtype=\"int64\")\n",
    "\n",
    "        # Conversion des npy array en torch.tensor\n",
    "        # A compléter\n",
    "        data = ...\n",
    "        label = ...\n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On partitionne les données en un jeu d'apprentissage et un jeu de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {'ignored_labels': [0], 'patch_size': 125}\n",
    "dataset = Dataset(img, gt, hyperparams)\n",
    "\n",
    "# A compléter\n",
    "train_dataset, val_dataset = ... # On souhaite couper le dataset en un set d'apprentissage et un set de validation\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut visualiser un exemple comme suit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for patch, label in train_loader:\n",
    "    break\n",
    "fig = plt.figure()\n",
    "plt.imshow(patch[0,:,:,:].transpose(2,0)/256)\n",
    "\n",
    "if label[0].item() == 1:\n",
    "    plt.title('Exemple de bâtiment')\n",
    "elif label[0].item() == 2:\n",
    "    plt.title('Exemple de voirie')\n",
    "else:\n",
    "    plt.title('Exemple autre')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning d'un modèle pré-entrainé\n",
    "\n",
    "On souhaiterait adapter le modèle AlexNet à nos données. En effet, le nombre de paramètres d'AlexNet est bien trop grand par rapport au nombre de pixels annotés sur notre image. Dans la suite, on conserve uniquement la première couche de convolution d'origine d'AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_rate=0):\n",
    "        super(TinyAlexNet, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 1, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(36, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def extract_features(self, inputs):\n",
    "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
    "        x = self.features(inputs)\n",
    "        return x\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.features(inputs)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyAlexNet(num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On charge les paramètres d'AlexNet optimisés sur ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "model = model.to(device) # On charge le modèle sur GPU si elle est disponible\n",
    "pretrained_weights = torch.load('../pretrained_alex_net.pth.tar', \n",
    "                                map_location=device) # on précise le \"device\" sur lequel est le modèle\n",
    "pretrained_weights['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On conserve les paramètres de la première couche de convolution, que l'on fige pour l'optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.features[0].weight.data = ...\n",
    "model.features[0].bias.data = ...\n",
    "del pretrained_weights # On supprime les poids de la mémoire du GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.features[0].weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A compléter\n",
    "... \n",
    "\n",
    "print(model.features[0].weight.requires_grad, model.features[0].bias.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons les dimensions de \"l'image\" au fil des convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones((1, 3, 125, 125))\n",
    "out = model.features[0](x)\n",
    "print(out.shape)\n",
    "out = model.features[2](out)\n",
    "print(out.shape)\n",
    "out = model.features[3](out)\n",
    "print(out.shape)\n",
    "out = model.features[5](out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit la technique d'optimisation et ses hyper-paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "learning_rate = 1e-4\n",
    "best_val = np.inf\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    y_pred, y_true = [], []\n",
    "    for patch, label in tqdm(train_loader):\n",
    "        # A compléter\n",
    "        ...\n",
    "        \n",
    "        y_pred.extend(torch.argmax(logits, dim=-1).cpu())\n",
    "        y_true.extend(label.cpu())\n",
    "          \n",
    "    train_accuracy = accuracy_score(y_pred, y_true)\n",
    "    train_f1 = f1_score(y_pred, y_true)\n",
    "    \n",
    "    print('[Train] \\t Loss: {:.4f} | Accuracy: {:.4f} | F1 score: {:.4f}'.format(\\\n",
    "                                            loss.item(), train_accuracy, train_f1))\n",
    "    \n",
    "    y_pred, y_true = [], []\n",
    "    for patch, label in tqdm(val_loader):\n",
    "        # A compléter \n",
    "        ...\n",
    "        \n",
    "        y_pred.extend(torch.argmax(logits, dim=-1).cpu())\n",
    "        y_true.extend(label.cpu())\n",
    "        \n",
    "    val_accuracy = accuracy_score(y_pred, y_true)\n",
    "    val_f1 = f1_score(y_pred, y_true)\n",
    "    \n",
    "    if val_loss.item() < best_val:\n",
    "        best_val = val_loss.item()\n",
    "        torch.save({'epoch': epoch, \n",
    "                    'best_loss': best_val, \n",
    "                    'state_dict': model.state_dict()}, 'best_model.pth.tar')\n",
    "        \n",
    "    \n",
    "    print('[Val] \\t Loss: {:.4f} | Accuracy: {:.4f} | F1 score: {:.4f}'.format(\\\n",
    "                                          val_loss.item(), val_accuracy, val_f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
