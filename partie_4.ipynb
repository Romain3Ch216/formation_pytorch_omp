{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formation PyTorch : les bases pour être autonome \n",
    "#### 3 novembre 2022 de 9h à 17h à l'OMP (salle Coriolis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 4\n",
    "## Modélisation probabiliste avec PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette dernière partie, nous allons nous initier à la modélisation probabiliste avec PyTorch avec un exemple de régression linéaire bayésienne. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.linspace(0, 10, 100).unsqueeze(1)\n",
    "Y = 2.5 + 3*X + 3*torch.randn((100,1))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plutôt que de calculer le maximum de vraisemblance $w^\\ast$ des paramètres du modèle linéaire (régression linéaire classique), l'objectif d'une régression linéaire bayésienne est de calculer la distribution des paramètres du modèle linéaire. On modélise la vraisemblance des données $p(y \\vert x, w)$ et l'a priori sur les paramètres du modèle $p(w)$ ainsi :\n",
    "\n",
    "$$p(y \\vert x, w) = \\mathcal{N}(y \\vert x^T w_1 + w_0, \\sigma)$$\n",
    "$$p(w) = \\mathcal{N}(w \\vert \\mu_\\circ, \\sigma_\\circ)$$\n",
    "\n",
    "Dans le cas présent, on pourrait utiliser la loi de Bayes pour calculer analytiquement la distribution a posteriori $p(w \\vert x, y)$. Néanmoins, pour des problèmes non linéaires, il n'y a souvent pas de solutions analytiques. \n",
    "\n",
    "Ici, on propose d'implémenter avec PyTorch une approche variationnelle (https://en.wikipedia.org/wiki/Variational_Bayesian_methods) qui consiste à approximer la distribution a posteriori par une distribution $q_\\phi(w)$ paramétrée par $\\phi$. Dans notre cas, $\\phi = [\\mu(w_0), \\sigma(w_0), \\mu(w_1), \\sigma(w_1)]$, les moyennes et les écarts-types de l'ordonnée à l'origine et du coefficient directeur.\n",
    "\n",
    "La fonction objective est une borne supérieure de la log-vraisemblance négative -log $p(x,y)$ : \n",
    "$$\\mathcal{L}(x,y) = - \\mathbb{E}_{q_\\phi(w)} \\big[\\mbox{log} \\: p(y \\vert x, w)\\big] + D_{KL}\\big(q_\\phi(w) \\vert\\vert p(w)\\big)$$\n",
    "\n",
    "où $D_{KL}$ désigne la divergence de Kullback-Leibler (https://fr.wikipedia.org/wiki/Divergence_de_Kullback-Leibler), que l'on peut calculer avec PyTorch (https://pytorch.org/docs/stable/distributions.html?highlight=torch+distributions+kl#module-torch.distributions.kl).\n",
    "\n",
    "Comme la vraisemblance est gaussienne, minimiser le terme - log $p(y \\vert x, w)$ revient à minismer l'erreur au carré entre les vrais $y_i$ et les $\\hat{y}_i$ prédits. \n",
    "\n",
    "Par ailleurs, on approxime l'espérance $\\mathbb{E}_{q_\\phi(w)}$ avec des échantillons de Monte Carlo comme suit : \n",
    "$$ \\mathbb{E}_{q_\\phi(w)} \\big[\\mbox{log} \\: p(y \\vert x, w)\\big] \\approx \\frac{1}{K} \\sum_k \\mbox{log} \\: p(y \\vert x, w^k)$$\n",
    "où $w^k \\sim q_\\phi(w)$. En pratique, on prend $K=1$.\n",
    "\n",
    "Ainsi, le modèle linéaire Bayésien est stochastique. Pour efficacement rétro-propager des gradients à travers des couches stochastiques, on va utiliser le \"reparametrization trick\" introduit dans [1]. Au lieu d'échantillonner $w^k$ à partir de $q_\\phi(w)$, on échantillonne un bruit $\\epsilon^k$ indépendant de $\\phi$:\n",
    "\n",
    "$$\\epsilon^k \\sim \\mathcal{N}(0,1)$$\n",
    "$$w^k = \\mu(w) + \\epsilon^k \\cdot \\sigma(w)$$\n",
    "\n",
    "[1] Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLinearModel(torch.nn.Module):\n",
    "    def __init__(self, x_dim):\n",
    "        super(BayesianLinearModel, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        # A compléter : définir q_\\phi(w)\n",
    "        ...\n",
    "        \n",
    "    def IC(self, level=0.95):\n",
    "        \"\"\"\n",
    "        Calcule l'intervalle de confiance de niveau 'level'\n",
    "        pour les paramètres du modèle.\n",
    "        \"\"\"\n",
    "        # A compléter \n",
    "        ...\n",
    "        return low, high\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # A compléter : utiliser le \"reparametrization trick\"\n",
    "        ... \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BayesianLinearModel(X.shape[1])\n",
    "prior = torch.distributions.normal.Normal(torch.zeros(2,1), 4*torch.ones(2,1))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "beta = 1e-3\n",
    "\n",
    "for epoch in range(20):\n",
    "    y_pred = model(X)\n",
    "    posterior = ...\n",
    "    kld = torch.distributions.kl.kl_divergence(posterior, prior).sum()\n",
    "    mse = F.mse_loss(Y, y_pred)\n",
    "    loss = mse + beta*kld\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f'=== Epoch {epoch} ===  ')\n",
    "    print(f'MSE: {mse.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_w, high_w = model.IP()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # A compléter\n",
    "    y_mean = ...\n",
    "    y_low = ...\n",
    "    y_high = ...\n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.scatter(X, Y)\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.fill_between(X.view(-1), y_low.view(-1), y_high.view(-1), alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
